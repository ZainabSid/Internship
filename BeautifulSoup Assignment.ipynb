{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa7f330c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Headers\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you knowÂ ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "df = pd.DataFrame({'Headers': [header.text.strip() for header in headers]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcf7f734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "presidents_data = []\n",
    "\n",
    "for president in soup.find_all('tr')[1:]:\n",
    "    columns = president.find_all('td')\n",
    "    name = columns[0].text.strip()\n",
    "    term_of_office = columns[1].text.strip()\n",
    "    presidents_data.append({'Name': name, 'Term of Office': term_of_office})\n",
    "\n",
    "df = pd.DataFrame(presidents_data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac6dd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Women's ODI Teams:\n",
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      21  3,429    163\n",
      "1      England\\nENG      23  2,991    130\n",
      "2  South Africa\\nSA      21  2,446    116\n",
      "3        India\\nIND      18  1,745     97\n",
      "4   New Zealand\\nNZ      21  2,014     96\n",
      "5   West Indies\\nWI      20  1,768     88\n",
      "6     Sri Lanka\\nSL       9    714     79\n",
      "7   Bangladesh\\nBAN      14  1,074     77\n",
      "8     Thailand\\nTHA      11    753     68\n",
      "9     Pakistan\\nPAK      24  1,602     67\n"
     ]
    }
   ],
   "source": [
    "def scrape_womens_odi_teams(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_womens_odi_teams_data(soup):\n",
    "    teams_data = []\n",
    "\n",
    "    table = soup.find('table', class_='table')\n",
    "    rows = table.find_all('tr')[1:11]  # Get the top 10 rows (excluding header)\n",
    "\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        team = columns[1].text.strip()\n",
    "        matches = columns[2].text.strip()\n",
    "        points = columns[3].text.strip()\n",
    "        rating = columns[4].text.strip()\n",
    "\n",
    "        teams_data.append({'Team': team, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "\n",
    "    return pd.DataFrame(teams_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "    \n",
    "    # Scrape and get data for Women's ODI Teams\n",
    "    womens_odi_teams_soup = scrape_womens_odi_teams(url)\n",
    "    womens_odi_teams_df = get_womens_odi_teams_data(womens_odi_teams_soup)\n",
    "\n",
    "    print(\"Top 10 Women's ODI Teams:\")\n",
    "    print(womens_odi_teams_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0217ed58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tables found: 1\n",
      "Table found, but no data to display.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_womens_odi_batting_players(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_womens_odi_batting_players_data(soup):\n",
    "    players_data = []\n",
    "\n",
    "    tables = soup.find_all('table', class_='table')\n",
    "    \n",
    "    if len(tables) >= 2:\n",
    "        table = tables[1]\n",
    "        rows = table.find_all('tr')[1:11]  # Get the top 10 rows (excluding header)\n",
    "\n",
    "        for row in rows:\n",
    "            columns = row.find_all('td')\n",
    "            player = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "\n",
    "            players_data.append({'Player': player, 'Team': team, 'Rating': rating})\n",
    "        \n",
    "        return pd.DataFrame(players_data)\n",
    "    else:\n",
    "        print(\"Number of tables found:\", len(tables))\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "    \n",
    "    # Scrape and get data for Women's ODI Batting Players\n",
    "    womens_odi_batting_players_soup = scrape_womens_odi_batting_players(url)\n",
    "    \n",
    "    if womens_odi_batting_players_soup:\n",
    "        womens_odi_batting_players_df = get_womens_odi_batting_players_data(womens_odi_batting_players_soup)\n",
    "\n",
    "        if womens_odi_batting_players_df is not None:\n",
    "            print(\"Top 10 Women's ODI Batting Players:\")\n",
    "            print(womens_odi_batting_players_df)\n",
    "        else:\n",
    "            print(\"Table found, but no data to display.\")\n",
    "    else:\n",
    "        print(\"Failed to fetch data from the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25c1cb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tables found: 1\n",
      "Table found, but no data to display.\n"
     ]
    }
   ],
   "source": [
    "def scrape_womens_odi_allrounders(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_womens_odi_allrounders_data(soup):\n",
    "    allrounders_data = []\n",
    "\n",
    "    tables = soup.find_all('table', class_='table')\n",
    "    \n",
    "    if len(tables) >= 2:\n",
    "        table = tables[1]\n",
    "        rows = table.find_all('tr')[1:11]  # Get the top 10 rows (excluding header)\n",
    "\n",
    "        for row in rows:\n",
    "            columns = row.find_all('td')\n",
    "            allrounder = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "\n",
    "            allrounders_data.append({'All-Rounder': allrounder, 'Team': team, 'Rating': rating})\n",
    "        \n",
    "        return pd.DataFrame(allrounders_data)\n",
    "    else:\n",
    "        print(\"Number of tables found:\", len(tables))\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "    \n",
    "    # Scrape and get data for Women's ODI All-Rounders\n",
    "    womens_odi_allrounders_soup = scrape_womens_odi_allrounders(url)\n",
    "    \n",
    "    if womens_odi_allrounders_soup:\n",
    "        womens_odi_allrounders_df = get_womens_odi_allrounders_data(womens_odi_allrounders_soup)\n",
    "\n",
    "        if womens_odi_allrounders_df is not None:\n",
    "            print(\"Top 10 Women's ODI All-Rounders:\")\n",
    "            print(womens_odi_allrounders_df)\n",
    "        else:\n",
    "            print(\"Table found, but no data to display.\")\n",
    "    else:\n",
    "        print(\"Failed to fetch data from the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13d809d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 CNBC World News:\n",
      "                                            Headline Time  \\\n",
      "0  S&P 500 rises on Friday to close at 2023 high:...  N/A   \n",
      "1  IMF chief makes the case for carbon pricing as...  N/A   \n",
      "2  John Kerry responds to COP28 president's claim...  N/A   \n",
      "3  Exxon Mobil CEO urges COP28 climate summit to ...  N/A   \n",
      "4  Bill Gates warns the world is likely to oversh...  N/A   \n",
      "5  UAE commits $30 billion to new climate-focused...  N/A   \n",
      "6  Op-ed: With continued geopolitical conflicts, ...  N/A   \n",
      "7  Zelenskyy says 'new phase of war' has begun, H...  N/A   \n",
      "8  Multiple civilians injured in strikes on Ukrai...  N/A   \n",
      "9  Russia slams Finland's border closure, warns t...  N/A   \n",
      "\n",
      "                                           News Link  \n",
      "0  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "1  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "2  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "3  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "4  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "5  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "6  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "7  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "8  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n",
      "9  https://www.cnbc.comhttps://www.cnbc.com/2023/...  \n"
     ]
    }
   ],
   "source": [
    "def scrape_cnbc_news(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_cnbc_news_data(soup):\n",
    "    news_data = []\n",
    "\n",
    "    articles = soup.find_all('div', class_='Card-titleContainer')[:10]  # Get the top 10 articles\n",
    "\n",
    "    for article in articles:\n",
    "        headline = article.find('a').text.strip()\n",
    "        \n",
    "        # Check if the 'time' tag is present\n",
    "        time_tag = article.find('time')\n",
    "        time = time_tag.text.strip() if time_tag else \"N/A\"\n",
    "\n",
    "        news_link = 'https://www.cnbc.com' + article.find('a')['href']\n",
    "\n",
    "        news_data.append({'Headline': headline, 'Time': time, 'News Link': news_link})\n",
    "\n",
    "    return pd.DataFrame(news_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://www.cnbc.com/world/?region=world'\n",
    "    \n",
    "    # Scrape and get data for CNBC News\n",
    "    cnbc_news_soup = scrape_cnbc_news(url)\n",
    "    \n",
    "    if cnbc_news_soup:\n",
    "        cnbc_news_df = get_cnbc_news_data(cnbc_news_soup)\n",
    "\n",
    "        if not cnbc_news_df.empty:\n",
    "            print(\"Top 10 CNBC World News:\")\n",
    "            print(cnbc_news_df)\n",
    "        else:\n",
    "            print(\"No data to display.\")\n",
    "    else:\n",
    "        print(\"Failed to fetch data from the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d8cf08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data to display.\n"
     ]
    }
   ],
   "source": [
    "def scrape_most_downloaded_articles(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_most_downloaded_articles_data(soup):\n",
    "    articles_data = []\n",
    "\n",
    "    articles = soup.find_all('div', class_='pod-listing')\n",
    "    \n",
    "    for article in articles:\n",
    "        title = article.find('h2').text.strip()\n",
    "        authors = article.find('div', class_='text-xs').text.strip()\n",
    "        published_date = article.find('div', class_='pod-listing__published-date').text.strip()\n",
    "        paper_url = 'https://www.journals.elsevier.com' + article.find('a')['href']\n",
    "\n",
    "        articles_data.append({'Paper Title': title, 'Authors': authors, 'Published Date': published_date, 'Paper URL': paper_url})\n",
    "\n",
    "    return pd.DataFrame(articles_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "    \n",
    "    # Scrape and get data for most downloaded articles in AI\n",
    "    most_downloaded_articles_soup = scrape_most_downloaded_articles(url)\n",
    "    \n",
    "    if most_downloaded_articles_soup:\n",
    "        most_downloaded_articles_df = get_most_downloaded_articles_data(most_downloaded_articles_soup)\n",
    "\n",
    "        if not most_downloaded_articles_df.empty:\n",
    "            print(\"Most Downloaded Articles in AI (Last 90 Days):\")\n",
    "            print(most_downloaded_articles_df)\n",
    "        else:\n",
    "            print(\"No data to display.\")\n",
    "    else:\n",
    "        print(\"Failed to fetch data from the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6c23860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data to display.\n"
     ]
    }
   ],
   "source": [
    "def scrape_dineout_details(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_dineout_data(soup):\n",
    "    restaurant_data = []\n",
    "\n",
    "    restaurants = soup.find_all('div', class_='restnt-info-wrap')\n",
    "    \n",
    "    for restaurant in restaurants:\n",
    "        name = restaurant.find('div', class_='restnt-info-wrap-section').find('h3').text.strip()\n",
    "        cuisine = restaurant.find('div', class_='restnt-info-wrap-section').find('p', class_='double-line-ellipsis').text.strip()\n",
    "        location = restaurant.find('div', class_='restnt-info-wrap-section').find('span', class_='double-line-ellipsis').text.strip()\n",
    "        ratings = restaurant.find('span', class_='img-wrap')['style'].split(':')[1].split('%')[0].strip()\n",
    "        image_url = restaurant.find('img', class_='no-error')['data-src']\n",
    "\n",
    "        restaurant_data.append({\n",
    "            'Restaurant Name': name,\n",
    "            'Cuisine': cuisine,\n",
    "            'Location': location,\n",
    "            'Ratings': ratings,\n",
    "            'Image URL': image_url\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(restaurant_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://www.dineout.co.in/delhi-restaurants/buffet-special'\n",
    "    \n",
    "    # Scrape and get data for restaurants from dineout.co.in\n",
    "    dineout_soup = scrape_dineout_details(url)\n",
    "    \n",
    "    if dineout_soup:\n",
    "        dineout_df = get_dineout_data(dineout_soup)\n",
    "\n",
    "        if not dineout_df.empty:\n",
    "            print(\"Restaurant Details from Dineout:\")\n",
    "            print(dineout_df)\n",
    "        else:\n",
    "            print(\"No data to display.\")\n",
    "    else:\n",
    "        print(\"Failed to fetch data from the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475a5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
