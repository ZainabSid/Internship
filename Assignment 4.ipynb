{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da987b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (836155201.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install requests\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install requests\n",
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e57feb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\zaina\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zaina\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zaina\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zaina\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zaina\\anaconda3\\lib\\site-packages (from requests) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\zaina\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\zaina\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests\n",
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb87d75f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Extract data from each column in the row\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m     rank \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     29\u001b[0m     name \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     30\u001b[0m     artist \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the most viewed videos\n",
    "    table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "    # Initialize lists to store the details\n",
    "    ranks = []\n",
    "    names = []\n",
    "    artists = []\n",
    "    upload_dates = []\n",
    "    views = []\n",
    "\n",
    "    # Iterate through the rows of the table (skipping the header row)\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        # Extract data from each column in the row\n",
    "        columns = row.find_all('td')\n",
    "        rank = columns[0].text.strip()\n",
    "        name = columns[1].text.strip()\n",
    "        artist = columns[2].text.strip()\n",
    "        upload_date = columns[3].text.strip()\n",
    "        view_count = columns[4].text.strip()\n",
    "\n",
    "        # Append data to the lists\n",
    "        ranks.append(rank)\n",
    "        names.append(name)\n",
    "        artists.append(artist)\n",
    "        upload_dates.append(upload_date)\n",
    "        views.append(view_count)\n",
    "\n",
    "    # Print or further process the scraped data\n",
    "    for i in range(len(ranks)):\n",
    "        print(f\"A) Rank: {ranks[i]}\")\n",
    "        print(f\"B) Name: {names[i]}\")\n",
    "        print(f\"C) Artist: {artists[i]}\")\n",
    "        print(f\"D) Upload Date: {upload_dates[i]}\")\n",
    "        print(f\"E) Views: {views[i]}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f13183fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row: <tr>\n",
      "<th style=\"background-position:center;line-height:100%;\"> </th>\n",
      "<th style=\"background-position:center\"> </th>\n",
      "<th style=\"background-position:center\"> </th>\n",
      "<th style=\"background-position:center\"> </th>\n",
      "<th style=\"background-position:center\">\n",
      "</th></tr>\n",
      "Skipping row: <tr>\n",
      "<th colspan=\"7\" style=\"text-align:center; font-size:8pt;\">As of December 19, 2023\n",
      "</th></tr>\n",
      "A) Rank: \"Baby Shark Dance\"[6]\n",
      "B) Name: Pinkfong Baby Shark - Kids' Songs & Stories\n",
      "C) Artist: 13.78\n",
      "D) Upload Date: June 17, 2016\n",
      "E) Views: [A]\n",
      "\n",
      "\n",
      "A) Rank: \"Despacito\"[9]\n",
      "B) Name: Luis Fonsi\n",
      "C) Artist: 8.33\n",
      "D) Upload Date: January 12, 2017\n",
      "E) Views: [B]\n",
      "\n",
      "\n",
      "A) Rank: \"Johny Johny Yes Papa\"[17]\n",
      "B) Name: LooLoo Kids - Nursery Rhymes and Children's Songs\n",
      "C) Artist: 6.85\n",
      "D) Upload Date: October 8, 2016\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Bath Song\"[18]\n",
      "B) Name: Cocomelon - Nursery Rhymes\n",
      "C) Artist: 6.52\n",
      "D) Upload Date: May 2, 2018\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Shape of You\"[19]\n",
      "B) Name: Ed Sheeran\n",
      "C) Artist: 6.15\n",
      "D) Upload Date: January 30, 2017\n",
      "E) Views: [C]\n",
      "\n",
      "\n",
      "A) Rank: \"See You Again\"[22]\n",
      "B) Name: Wiz Khalifa\n",
      "C) Artist: 6.11\n",
      "D) Upload Date: April 6, 2015\n",
      "E) Views: [D]\n",
      "\n",
      "\n",
      "A) Rank: \"Wheels on the Bus\"[27]\n",
      "B) Name: Cocomelon - Nursery Rhymes\n",
      "C) Artist: 5.74\n",
      "D) Upload Date: May 24, 2018\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Phonics Song with Two Words\"[28]\n",
      "B) Name: ChuChu TV Nursery Rhymes & Kids Songs\n",
      "C) Artist: 5.59\n",
      "D) Upload Date: March 6, 2014\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Uptown Funk\"[29]\n",
      "B) Name: Mark Ronson\n",
      "C) Artist: 5.10\n",
      "D) Upload Date: November 19, 2014\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Learning Colors – Colorful Eggs on a Farm\"[30]\n",
      "B) Name: Miroshka TV\n",
      "C) Artist: 5.03\n",
      "D) Upload Date: February 27, 2018\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Gangnam Style\"[31]\n",
      "B) Name: Psy\n",
      "C) Artist: 4.98\n",
      "D) Upload Date: July 15, 2012\n",
      "E) Views: [E]\n",
      "\n",
      "\n",
      "A) Rank: \"Masha and the Bear – Recipe for Disaster\"[36]\n",
      "B) Name: Get Movies\n",
      "C) Artist: 4.57\n",
      "D) Upload Date: January 31, 2012\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Dame Tu Cosita\"[37]\n",
      "B) Name: Ultra Records\n",
      "C) Artist: 4.49\n",
      "D) Upload Date: April 5, 2018\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Axel F\"[38]\n",
      "B) Name: Crazy Frog\n",
      "C) Artist: 4.19\n",
      "D) Upload Date: June 16, 2009\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Sugar\"[39]\n",
      "B) Name: Maroon 5\n",
      "C) Artist: 3.97\n",
      "D) Upload Date: January 14, 2015\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Counting Stars\"[40]\n",
      "B) Name: OneRepublic\n",
      "C) Artist: 3.93\n",
      "D) Upload Date: May 31, 2013\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Roar\"[41]\n",
      "B) Name: Katy Perry\n",
      "C) Artist: 3.92\n",
      "D) Upload Date: September 5, 2013\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Baa Baa Black Sheep\"[42]\n",
      "B) Name: Cocomelon - Nursery Rhymes\n",
      "C) Artist: 3.86\n",
      "D) Upload Date: June 25, 2018\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Waka Waka (This Time for Africa)\"[43]\n",
      "B) Name: Shakira\n",
      "C) Artist: 3.80\n",
      "D) Upload Date: June 4, 2010\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Lakdi Ki Kathi\"[44]\n",
      "B) Name: Jingle Toons\n",
      "C) Artist: 3.78\n",
      "D) Upload Date: June 14, 2018\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Sorry\"[45]\n",
      "B) Name: Justin Bieber\n",
      "C) Artist: 3.74\n",
      "D) Upload Date: October 22, 2015\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Thinking Out Loud\"[46]\n",
      "B) Name: Ed Sheeran\n",
      "C) Artist: 3.70\n",
      "D) Upload Date: October 7, 2014\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Humpty the train on a fruits ride\"[47]\n",
      "B) Name: Kiddiestv Hindi - Nursery Rhymes & Kids Songs\n",
      "C) Artist: 3.65\n",
      "D) Upload Date: January 26, 2018\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Dark Horse\"[48]\n",
      "B) Name: Katy Perry\n",
      "C) Artist: 3.64\n",
      "D) Upload Date: February 20, 2014\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Perfect\"[49]\n",
      "B) Name: Ed Sheeran\n",
      "C) Artist: 3.61\n",
      "D) Upload Date: November 9, 2017\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Shree Hanuman Chalisa\"[50]\n",
      "B) Name: T-Series Bhakti Sagar\n",
      "C) Artist: 3.57\n",
      "D) Upload Date: May 10, 2011\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Let Her Go\"[51]\n",
      "B) Name: Passenger\n",
      "C) Artist: 3.57\n",
      "D) Upload Date: July 25, 2012\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Faded\"[52]\n",
      "B) Name: Alan Walker\n",
      "C) Artist: 3.56\n",
      "D) Upload Date: December 3, 2015\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Girls Like You\"[53]\n",
      "B) Name: Maroon 5\n",
      "C) Artist: 3.53\n",
      "D) Upload Date: May 31, 2018\n",
      "E) Views: \n",
      "\n",
      "\n",
      "A) Rank: \"Lean On\"[54]\n",
      "B) Name: Major Lazer Official\n",
      "C) Artist: 3.51\n",
      "D) Upload Date: March 22, 2015\n",
      "E) Views: \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the most viewed videos\n",
    "    table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "    # Initialize lists to store the details\n",
    "    ranks = []\n",
    "    names = []\n",
    "    artists = []\n",
    "    upload_dates = []\n",
    "    views = []\n",
    "\n",
    "    # Iterate through the rows of the table (skipping the header row)\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        # Extract data from each column in the row\n",
    "        columns = row.find_all('td')\n",
    "\n",
    "        # Check if there are enough columns in the row\n",
    "        if len(columns) >= 5:\n",
    "            rank = columns[0].text.strip()\n",
    "            name = columns[1].text.strip()\n",
    "            artist = columns[2].text.strip()\n",
    "            upload_date = columns[3].text.strip()\n",
    "            view_count = columns[4].text.strip()\n",
    "\n",
    "            # Append data to the lists\n",
    "            ranks.append(rank)\n",
    "            names.append(name)\n",
    "            artists.append(artist)\n",
    "            upload_dates.append(upload_date)\n",
    "            views.append(view_count)\n",
    "        else:\n",
    "            print(f\"Skipping row: {row}\")\n",
    "\n",
    "    # Print or further process the scraped data\n",
    "    for i in range(len(ranks)):\n",
    "        print(f\"A) Rank: {ranks[i]}\")\n",
    "        print(f\"B) Name: {names[i]}\")\n",
    "        print(f\"C) Artist: {artists[i]}\")\n",
    "        print(f\"D) Upload Date: {upload_dates[i]}\")\n",
    "        print(f\"E) Views: {views[i]}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "434578ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find the link to the international fixtures page on the homepage.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Send a GET request to the BCCI homepage\n",
    "homepage_url = \"https://www.bcci.tv/\"\n",
    "response_homepage = requests.get(homepage_url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response_homepage.status_code == 200:\n",
    "    # Step 2: Parse the HTML content of the homepage\n",
    "    soup_homepage = BeautifulSoup(response_homepage.text, 'html.parser')\n",
    "\n",
    "    # Step 3: Find the link to the international fixtures page\n",
    "    fixtures_link = soup_homepage.find('a', {'href': '/international/fixtures'})\n",
    "    \n",
    "    if fixtures_link:\n",
    "        # Step 4: Construct the URL for the international fixtures page\n",
    "        fixtures_url = f\"https://www.bcci.tv{fixtures_link['href']}\"\n",
    "\n",
    "        # Step 5: Send a GET request to the international fixtures page\n",
    "        response_fixtures = requests.get(fixtures_url)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response_fixtures.status_code == 200:\n",
    "            # Step 6: Parse the HTML content of the fixtures page\n",
    "            soup_fixtures = BeautifulSoup(response_fixtures.text, 'html.parser')\n",
    "\n",
    "            # Step 7: Find and extract the details of Team India's international fixtures\n",
    "            fixtures_data = []\n",
    "            fixtures_container = soup_fixtures.find('div', {'class': 'js-list'})\n",
    "\n",
    "            for fixture in fixtures_container.find_all('li'):\n",
    "                series = fixture.find('p', {'class': 'fixture__additional-info'}).text.strip()\n",
    "                place = fixture.find('p', {'class': 'fixture__additional-info'}).find_next('strong').text.strip()\n",
    "                date = fixture.find('div', {'class': 'fixture__full-date'}).text.strip()\n",
    "                time = fixture.find('div', {'class': 'fixture__date-details'}).text.strip()\n",
    "\n",
    "                fixtures_data.append({\n",
    "                    'A) Series': series,\n",
    "                    'B) Place': place,\n",
    "                    'C) Date': date,\n",
    "                    'D) Time': time\n",
    "                })\n",
    "\n",
    "            # Step 8: Print or further process the scraped data\n",
    "            for fixture_data in fixtures_data:\n",
    "                print(f\"A) Series: {fixture_data['A) Series']}\")\n",
    "                print(f\"B) Place: {fixture_data['B) Place']}\")\n",
    "                print(f\"C) Date: {fixture_data['C) Date']}\")\n",
    "                print(f\"D) Time: {fixture_data['D) Time']}\")\n",
    "                print(\"\\n\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the fixtures page. Status code: {response_fixtures.status_code}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Could not find the link to the international fixtures page on the homepage.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve the homepage. Status code: {response_homepage.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed5bea9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (4213106765.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    driver = webdriver.Chrome('C:\\Users\\Zaina\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe')\u001b[0m\n\u001b[1;37m                                                                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up the Chrome driver\n",
    "driver = webdriver.Chrome('C:\\Users\\Zaina\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe')\n",
    "\n",
    "# Step 1: Open the BCCI homepage\n",
    "homepage_url = \"https://www.bcci.tv/\"\n",
    "driver.get(homepage_url)\n",
    "\n",
    "# Step 2: Find and click on the link to the international fixtures page\n",
    "fixtures_link = driver.find_element_by_xpath('//a[@href=\"/international/fixtures\"]')\n",
    "fixtures_link.click()\n",
    "\n",
    "# Step 3: Wait for the page to load\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Step 4: Get the HTML content of the fixtures page\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Step 5: Find and extract the details of Team India's international fixtures\n",
    "fixtures_data = []\n",
    "fixtures_container = soup.find('div', {'class': 'js-list'})\n",
    "\n",
    "for fixture in fixtures_container.find_all('li'):\n",
    "    series = fixture.find('p', {'class': 'fixture__additional-info'}).text.strip()\n",
    "    place = fixture.find('p', {'class': 'fixture__additional-info'}).find_next('strong').text.strip()\n",
    "    date = fixture.find('div', {'class': 'fixture__full-date'}).text.strip()\n",
    "    time = fixture.find('div', {'class': 'fixture__date-details'}).text.strip()\n",
    "\n",
    "    fixtures_data.append({\n",
    "        'A) Series': series,\n",
    "        'B) Place': place,\n",
    "        'C) Date': date,\n",
    "        'D) Time': time\n",
    "    })\n",
    "\n",
    "# Step 6: Print or further process the scraped data\n",
    "for fixture_data in fixtures_data:\n",
    "    print(f\"A) Series: {fixture_data['A) Series']}\")\n",
    "    print(f\"B) Place: {fixture_data['B) Place']}\")\n",
    "    print(f\"C) Date: {fixture_data['C) Date']}\")\n",
    "    print(f\"D) Time: {fixture_data['D) Time']}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Step 7: Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dda7817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\zaina\\anaconda3\\lib\\site-packages (4.16.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\zaina\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\zaina\\anaconda3\\lib\\site-packages (from selenium) (0.23.1)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\zaina\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\zaina\\anaconda3\\lib\\site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\zaina\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\zaina\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\zaina\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\zaina\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\zaina\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\zaina\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\zaina\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\zaina\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\zaina\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Installing collected packages: sniffio\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.2.0\n",
      "    Uninstalling sniffio-1.2.0:\n",
      "      Successfully uninstalled sniffio-1.2.0\n",
      "Successfully installed sniffio-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc3b084e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'capabilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:38\u001b[0m, in \u001b[0;36mDriverFinder.get_path\u001b[1;34m(service, options)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     path \u001b[38;5;241m=\u001b[39m SeleniumManager()\u001b[38;5;241m.\u001b[39mdriver_location(options) \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\common\\selenium_manager.py:84\u001b[0m, in \u001b[0;36mSeleniumManager.driver_location\u001b[1;34m(self, options)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Determines the path of the correct driver.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m:Args:\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m - browser: which browser to get the driver path for.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m:Returns: The driver path to use\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m browser \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mcapabilities[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     86\u001b[0m args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_binary()), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--browser\u001b[39m\u001b[38;5;124m\"\u001b[39m, browser]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'capabilities'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Set up the Chrome driver\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mZaina\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mchromedriver-win64\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mchromedriver-win64\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mchromedriver.exe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Step 1: Open the BCCI homepage\u001b[39;00m\n\u001b[0;32m      8\u001b[0m homepage_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.bcci.tv/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[0;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     46\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mDesiredCapabilities\u001b[38;5;241m.\u001b[39mCHROME[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     47\u001b[0m     vendor_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoog\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     48\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m     49\u001b[0m     service\u001b[38;5;241m=\u001b[39mservice,\n\u001b[0;32m     50\u001b[0m     keep_alive\u001b[38;5;241m=\u001b[39mkeep_alive,\n\u001b[0;32m     51\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:49\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new WebDriver instance of the ChromiumDriver. Starts the\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03mservice and then creates new WebDriver instance of ChromiumDriver.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m - keep_alive - Whether to configure ChromiumRemoteConnection to use HTTP keep-alive.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m DriverFinder\u001b[38;5;241m.\u001b[39mget_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice, options)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     52\u001b[0m executor \u001b[38;5;241m=\u001b[39m ChromiumRemoteConnection(\n\u001b[0;32m     53\u001b[0m     remote_server_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mservice_url,\n\u001b[0;32m     54\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mbrowser_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     ignore_proxy\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m_ignore_local_proxy,\n\u001b[0;32m     58\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:40\u001b[0m, in \u001b[0;36mDriverFinder.get_path\u001b[1;34m(service, options)\u001b[0m\n\u001b[0;32m     38\u001b[0m     path \u001b[38;5;241m=\u001b[39m SeleniumManager()\u001b[38;5;241m.\u001b[39mdriver_location(options) \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m---> 40\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;241m.\u001b[39mcapabilities[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using Selenium Manager.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(path)\u001b[38;5;241m.\u001b[39mis_file():\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'capabilities'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up the Chrome driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\Zaina\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\")\n",
    "\n",
    "# Step 1: Open the BCCI homepage\n",
    "homepage_url = \"https://www.bcci.tv/\"\n",
    "driver.get(homepage_url)\n",
    "\n",
    "# Step 2: Find and click on the link to the international fixtures page\n",
    "fixtures_link = driver.find_element_by_xpath('//a[@href=\"/international/fixtures\"]')\n",
    "fixtures_link.click()\n",
    "\n",
    "# Step 3: Wait for the page to load\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Step 4: Get the HTML content of the fixtures page\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Step 5: Find and extract the details of Team India's international fixtures\n",
    "fixtures_data = []\n",
    "fixtures_container = soup.find('div', {'class': 'js-list'})\n",
    "\n",
    "for fixture in fixtures_container.find_all('li'):\n",
    "    series = fixture.find('p', {'class': 'fixture__additional-info'}).text.strip()\n",
    "    place = fixture.find('p', {'class': 'fixture__additional-info'}).find_next('strong').text.strip()\n",
    "    date = fixture.find('div', {'class': 'fixture__full-date'}).text.strip()\n",
    "    time = fixture.find('div', {'class': 'fixture__date-details'}).text.strip()\n",
    "\n",
    "    fixtures_data.append({\n",
    "        'A) Series': series,\n",
    "        'B) Place': place,\n",
    "        'C) Date': date,\n",
    "        'D) Time': time\n",
    "    })\n",
    "\n",
    "# Step 6: Print or further process the scraped data\n",
    "for fixture_data in fixtures_data:\n",
    "    print(f\"A) Series: {fixture_data['A) Series']}\")\n",
    "    print(f\"B) Place: {fixture_data['B) Place']}\")\n",
    "    print(f\"C) Date: {fixture_data['C) Date']}\")\n",
    "    print(f\"D) Time: {fixture_data['D) Time']}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Step 7: Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d80acb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m gdp_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m gdp_table \u001b[38;5;241m=\u001b[39m soup_economy\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable_id\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m gdp_table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m     20\u001b[0m     columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m     rank \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Directly construct the URL for the Economy page\n",
    "economy_url = \"https://statisticstimes.com/economy/india-statistics.php\"\n",
    "\n",
    "# Step 2: Send a GET request to the Economy page\n",
    "response_economy = requests.get(economy_url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response_economy.status_code == 200:\n",
    "    # Step 3: Parse the HTML content of the Economy page\n",
    "    soup_economy = BeautifulSoup(response_economy.text, 'html.parser')\n",
    "\n",
    "    # Step 4: Find and extract the details of State-wise GDP\n",
    "    gdp_data = []\n",
    "    gdp_table = soup_economy.find('table', {'id': 'table_id'})\n",
    "\n",
    "    for row in gdp_table.find_all('tr')[1:]:\n",
    "        columns = row.find_all('td')\n",
    "        rank = columns[0].text.strip()\n",
    "        state = columns[1].text.strip()\n",
    "        gsdp_18_19 = columns[2].text.strip()\n",
    "        gsdp_19_20 = columns[3].text.strip()\n",
    "        share_18_19 = columns[4].text.strip()\n",
    "        gdp_billion = columns[5].text.strip()\n",
    "\n",
    "        gdp_data.append({\n",
    "            'A) Rank': rank,\n",
    "            'B) State': state,\n",
    "            'C) GSDP(18-19)': gsdp_18_19,\n",
    "            'D) GSDP(19-20)': gsdp_19_20,\n",
    "            'E) Share(18-19)': share_18_19,\n",
    "            'F) GDP($ billion)': gdp_billion\n",
    "        })\n",
    "\n",
    "    # Step 5: Print or further process the scraped data\n",
    "    for data in gdp_data:\n",
    "        print(f\"A) Rank: {data['A) Rank']}\")\n",
    "        print(f\"B) State: {data['B) State']}\")\n",
    "        print(f\"C) GSDP(18-19): {data['C) GSDP(18-19)']}\")\n",
    "        print(f\"D) GSDP(19-20): {data['D) GSDP(19-20)']}\")\n",
    "        print(f\"E) Share(18-19): {data['E) Share(18-19)']}\")\n",
    "        print(f\"F) GDP($ billion): {data['F) GDP($ billion)']}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve the Economy page. Status code: {response_economy.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97275a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
